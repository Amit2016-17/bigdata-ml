{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Apache Spark - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook code supports the blog:  \n",
    "https://medium.com/expedia-group-tech/start-your-journey-with-apache-spark-part-2-682891efda4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logically, DataFrames are similar to relational tables or DataFrames in Python/R with lots of optimizations behind the scene. There are various ways we can create DataFrames from collections, HIVE tables, Relational tables, and RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (8, \"HHH\", \"dept3\", 3700),\n",
    "    (9, \"III\", \"dept3\", 4500),\n",
    "    (10, \"JJJ\", \"dept5\", 3400)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  8| HHH|dept3|  3700|\n",
      "|  9| III|dept3|  4500|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations on DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "* Count the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access the names of columns in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'dept', 'salary']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access the DataType of columns within the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('dept', 'string'),\n",
       " ('salary', 'bigint')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check how Spark stores the schema of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,LongType,true),StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,LongType,true)))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select\n",
    "* Select particular columns from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| AAA|\n",
      "|  2| BBB|\n",
      "|  3| CCC|\n",
      "|  4| DDD|\n",
      "|  5| EEE|\n",
      "|  6| FFF|\n",
      "|  7| GGG|\n",
      "|  8| HHH|\n",
      "|  9| III|\n",
      "| 10| JJJ|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "* Filter the rows based on some condition. \n",
    "* Let's try to find the rows with id = 1. \n",
    "* There are different ways to specify the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute below statement by uncommeting them.\n",
    "\n",
    "df.filter(df[\"id\"] == 1).show()\n",
    "#df.filter(df.id == 1).show()\n",
    "#df.filter(col(\"id\") == 1).show()\n",
    "#df.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop\n",
    "* Drop a particular Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| AAA|dept1|  1000|\n",
      "| BBB|dept1|  1100|\n",
      "+----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.drop(\"id\")\n",
    "newdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "* We can use the groupBy function to group the data and then use the \"agg\" function to perform aggregation on grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+----+----+------+\n",
      "| dept|count|  sum| max| min|   avg|\n",
      "+-----+-----+-----+----+----+------+\n",
      "|dept5|    1| 3400|3400|3400|3400.0|\n",
      "|dept3|    3|15300|7100|3700|5100.0|\n",
      "|dept1|    4| 6600|3000|1000|1650.0|\n",
      "|dept2|    2|15200|8000|7200|7600.0|\n",
      "+-----+-----+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy(\"dept\")\n",
    "    .agg(\n",
    "        count(\"salary\").alias(\"count\"),\n",
    "        sum(\"salary\").alias(\"sum\"),\n",
    "        max(\"salary\").alias(\"max\"),\n",
    "        min(\"salary\").alias(\"min\"),\n",
    "        avg(\"salary\").alias(\"avg\")\n",
    "        ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "\n",
    "* Sort the data based on \"salary\". By default, sorting will be done in Ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  3| CCC|dept1|  3000|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"salary\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  9| III|dept3|  4500|\n",
      "|  8| HHH|dept3|  3700|\n",
      "+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data in descending order.\n",
    "df.sort(desc(\"salary\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived Columns\n",
    "* We can use the \"withColumn\" function to derive the column based on existing columns…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+\n",
      "| id|name| dept|salary|bonus|\n",
      "+---+----+-----+------+-----+\n",
      "|  1| AAA|dept1|  1000|100.0|\n",
      "|  2| BBB|dept1|  1100|110.0|\n",
      "|  3| CCC|dept1|  3000|300.0|\n",
      "|  4| DDD|dept1|  1500|150.0|\n",
      "|  5| EEE|dept2|  8000|800.0|\n",
      "|  6| FFF|dept2|  7200|720.0|\n",
      "|  7| GGG|dept3|  7100|710.0|\n",
      "|  8| HHH|dept3|  3700|370.0|\n",
      "|  9| III|dept3|  4500|450.0|\n",
      "| 10| JJJ|dept5|  3400|340.0|\n",
      "+---+----+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"bonus\", col(\"salary\") * .1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "* We can perform various types of joins on multiple DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n",
      "| id|name| dept|salary|   id|          name|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|  9| III|dept3|  4500|dept3|Department - 3|\n",
      "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner JOIN.\n",
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n",
      "| id|name| dept|salary|   id|          name|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "| 10| JJJ|dept5|  3400| null|          null|\n",
      "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|  9| III|dept3|  4500|dept3|Department - 3|\n",
      "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n",
      "|  id|name| dept|salary|   id|          name|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|   9| III|dept3|  4500|dept3|Department - 3|\n",
      "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|null|null| null|  null|dept4|Department - 4|\n",
      "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n",
      "|  id|name| dept|salary|   id|          name|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "|  10| JJJ|dept5|  3400| null|          null|\n",
      "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|   9| III|dept3|  4500|dept3|Department - 3|\n",
      "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|null|null| null|  null|dept4|Department - 4|\n",
      "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Queries\n",
    "* Executing SQL like queries.\n",
    "* We can perform data analysis by writing SQL like queries as well. In order to perform the SQL like queries, we need to register the DataFrame as a Temporary View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as Temporary Table\n",
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "# Execute SQL-Like query.\n",
    "spark.sql(\"select * from temp_table where id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading HIVE table as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB_NAME : Name of the the HIVE Database\n",
    "# TBL_NAME : Name of the HIVE Table\n",
    "\n",
    "# Uncomment below statement and provide your Hive DB and Table details.\n",
    "#df = spark.table(\"DB_NAME\".\"TBL_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DataFrame as HIVE Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below statement and provide your Hive DB and Table details.\n",
    "#df.write.saveAsTable(\"DB_NAME.TBL_NAME\")\n",
    "\n",
    "# We can also select the \"mode\" argument for \"overwrite\", \"append\", \"error\" etc.\n",
    "# df.write.saveAsTable(\"DB_NAME.TBL_NAME\", mode=\"overwrite\")\n",
    "\n",
    "# Note: By default, the operation will save the DataFrame as a HIVE Managed/Internal table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the DataFrame as a HIVE External table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below statement and provide your Hive DB and Table details.\n",
    "# Also, provide the path where you would like to save the Data.\n",
    "\n",
    "# df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame from CSV file\n",
    "* We can create a DataFrame using a CSV file and can specify various options like a separator, header, schema, inferSchema, and various other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment below code and provide necessary details.\n",
    "# df = spark.read.csv(\"path_to_csv_file\", sep=\"|\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ref : https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a DataFrame as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment below code and provide necessary details.\n",
    "# df.write.csv(\"path_to_CSV_File\", sep=\"|\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ref : https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame from a relational table\n",
    "* We can read the data from relational databases using a JDBC URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
    "# TBL_NAME : Name of the relational table.\n",
    "# USER_NAME : user name to connect to DataBase.\n",
    "# PASSWORD: password to connect to DataBase.\n",
    "\n",
    "# Un-comment below code and provide necessary details.\n",
    "# relational_df = spark.read.format('jdbc')\n",
    "#                        .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
    "#                        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ref : https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the DataFrame as a relational table\n",
    "* We can save the DataFrame as a relational table using a JDBC URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
    "# TBL_NAME : Name of the relational table.\n",
    "# USER_NAME : user name to connect to DataBase.\n",
    "# PASSWORD: password to connect to DataBase.\n",
    "\n",
    "# Un-comment below code and provide necessary details.\n",
    "# relational_df.write.format('jdbc')\n",
    "#                    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
    "#                    .mode('overwrite')\n",
    "#                    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ref : https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.jdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank You - End of Part 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
